CNN
입력 #(batch_size, rows, columns, channels) 4차원
출력 4차원

model.add(Conv2D(7, (2,2), input_shape=(8,8,1)))
(batch_size, rows, columns, channels)
(kernal_size + bias)*input*output
(2*2+1)*1*7


DNN
입력 통상적으로 2차원
출력 통상적으로 2차원
model.add(Dense(64, input_shape=(784,)))
(784+1)*64


SimpleRNN
#[batch, timesteps, feature] 3차원
출력 2차원 (return_sequence 쓰면 3차원)
model.add(SimpleRNN(10, input_shape=(5, 1)))
units * (feature + bias + units) = params
(10 * 10) + (1 * 10) + 10 = 120


LSTM
입력 [batch, timesteps, feature] 3차원
출력 2차원 (return_sequence 쓰면 3차원)
model.add(LSTM(10, input_shape=(5, 1)))
4 * units * (feature + bias + units) = params
4 * {(10*10) + (1*10) + 10} = 480
RNN의 4배

GRU
#[batch, timesteps, feature] 3차원
출력 2차원(return_sequence 쓰면 3차원)

model.add(LSTM(10, input_shape=(5, 1)))
3 * units * (feature + bias + units ) = params
3 * {(10*10)+(1*10) + 10} = 360
...정석이지만 최신버전은 bias = 2
이기 때문에
3 * {(10*10) + (1*10) + 10} = 390
