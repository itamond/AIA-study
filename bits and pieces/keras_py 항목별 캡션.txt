


keras07_mlp1

행렬데이터를 이용한 모델 구성



keras07_mlp2

행/열 전환(전치행렬)  x= x.transpose() ,x= x.T  



keras07_mlp3

전치행렬을 이용한 모델 구성


keras07_mlp4

range 함수와 x.T, y.T를 이용한 모델 구성


keras07_mlp5

x는 3개 y는 2개  input 3개 output 2개를 이용한 모델 구성


keras07_mlp6

x는 3개 y는 3개를 이용한 모델 구성


keras07_mlp7

x는 1개 y는 3개를 이용한 모델 구성    **잘 쓰지 않는다**


keras08_train_test1

fit과 evaluate에 데이터를 나누어서 넣는법
train데이터와 test데이터를 나눈다.
x_train, y_train, x_test, y_test


keras08_train_test2

넘파이 리스트 슬라이싱을 이용한 자료 나누기
x[:7] = x의 첫번째부터 7번째까지 
x[7:] = x의 7번째 초과부터 마지막까지
x.shape<< 잘 모르겠으면 모양 항상 확인해보기


keras08_train_test3

**사이킷런** import sklearn 을 활용한 train,test 슬라이싱
train과 test를 'shuffle'해서 일정 비율로 뽑아내는 방법

import sklearn
x_train, x_test, y_train, y_test = train_test split(x, y, test_size=0.3, shuffle=True(디폴트가 True), random_state=x)
*random_state-> 랜덤 시드값. 랜덤이더라도 다음에 같은 환경에서 훈련할 수 있도록 잡아주는 시드
*test_size, train_size 어떤것으로 해도 상관 없다.

print('x_train shape:', x_train.shape) 등으로 모양을 확인하자

나누어진 x y train, test를 각각 evaluate(트레인)와 predict(테스트)에 활용한다.





keras09_scatter

맷플롯라이브러리-파이플롯을 이용한 간단한 데이터 시각화



keras10_R2

from sklearn.metrix import r2_score를 이용한 결정계수 R2 구하는법




keras11 

sklearn.datasets import load_boston, california, diabetes 를 이용한
모델 구성



keras12_verbose

verbose 함수를 이용한 fit, evaluate 아봉


keras13
dacon의 ddarung 데이터를 이용한 모델링
***결측치 제거법***
csv 입력시키는법, csv에서 x와 y 컬런을 따로 지정해 주는법
read_csv를 이용한 입력
파일명 뒤에 drop, dropna를 이용한 컬런 버리기 기법




keras14
캐글 바이크 모델링



keras15
validation에 관하여
훈련중 자체 테스트를 통해 w값을 조정한다.


keras16
overfit에 관해.
모델링을 통해 overfit을 눈으로 확인해봄



keras17
EarlyStopping 을 이용한 
최적의 w값 보존법


keras18
이진분류의 활성화 함수 sigmoid
이진분류의 로스값 산출 함수는 binary_crossentropy
최종 값은 0이나 1이어야 하므로 반올림 함수 round를 사용하여 0이나 1로 맞춰주어야 한다.



keras19
다중분류의 활성화 함수 softmax
부드러운 최대값이라는 뜻으로 각 컬런마다 확률을 부여하여 출력한다.
이 방식은 0.5 이하의 확률이 부여될 수도 있다. 때문에 round 함수는 사용하지 못하고
가장 높은 값을 출력해주는 argmax 함수를 사용하여 가장 높은 값을 출력한다.
이 방식을 사용하면 데이터는 수치가 아닌 class가 
되어야 하므로 수치로 분류하면 문제가 생긴다.
때문에 onehotencoder 를 통해 데이터를 수치가 
아닌 위치로 저장 시키는 preprocessing(전처리)과정을 거쳐 문제가 없다.


keras20
argmax의 계산법에 관하여.
axis=0은 행끼리 비교, axis=1은 열끼리 비교 시킨다.


keras21
onehot인코더에 관해 조사했다


keras22
summary 함수에 관하여.
서머리 함수는 모델의 각 노드 파라미터의 합을 보여준다.
이 때에는 각 노드마다 bias를 합치는 작업도 하기때문에 각 계산횟수+노드의 갯수가 된다.


keras23
scaler에 관하여
데이터가 커지면 커질수록 컴퓨터 메모리에 부담이 가고, 실제로 너무 커지면 시스템이 다운된다.
때문에 데이터를 비율대로 줄여줄 필요가 있는데 이것이 scaler.
단순히 크기만 줄여 부담을 줄이고 속도를 올리는 것 뿐만 아니라 성능 향상도 노려볼 수 있다.
자세한 내용은 23번으로
스케일러는 항상 내가 원하는 사이즈를 갖도록 fit 해준 후
변환할 데이터에 transform을 적용해야한다.
스케일러는 일반적으로 훈련 데이터에만 정규화 한다. 과적합 때문.
x_train의 스케일 비율대로 test도 스케일 해준다.


keras24
input_dim 을 써서 단순히 차원만 입력했던 지금까지와 달리
input_shape로 모양 자체로 모델에 입력하는 방법을 배움.


keras25
함수형 모델에 관해 배움
Input 레이어를 만든 후 각 레이어마다 연결해주고
아웃풋 레이어 명시, 최종적으로 모델에 대한 정의를 한다.


keras26
모델의 저장과 불러오기에 관한 내용
저장하는 위치, 불러오는 위치에 따라 적용 방식이 다르다.


keras27
ModelChekpoint에 관해 배운다.
기본 골자는 es와 비슷하나, 모델의 최적의 w값을 파일로 저장할 수 있다.


keras28
dropout 레이어에 관해 배움
모델의 훈련때 오버핏을 방지하기 위해 훈련 자체 노드수를 줄여서 훈련시키는 방법
이 방법이 항상 좋다고 할 수는 없으나 하이퍼파라미터 튜닝의 한 종류이다


keras29
LabelEncoder에 관해
범주형 데이터를 숫자로 바꾸는 방법에 관해 배움
스케일러와 같이 fit과 transform을 통해 작동시킨다.


keras30
뉴럴네트워크의 한 종류인 CNN에 관해 배움.
일반적으로 이미지를 데이터로 사용할때 쓴다.
2D사진을 사용하면 Conv2D를 사용.
이는 4차원 데이터를 받아 4차원 데이터를 내보낸다.
따라서 겹쳐서 사용 가능하고 사용할 수 있는 하이퍼 파라미터는
padding = same, valid
maxpooling ,
kernel_size,
filters,
activation 등 다양하다.
이를 사용할 때 4차원 아웃풋을 가진 cnn 레이어를, 최종적으로 Dense 레이어로 배출하기위해
중간에 Flatten 레이어를 넣어 이미지를 평평히 펴주는 작업이 필요하다.


keras31
패딩, 스트라이드에 관해 배움
패딩은 컨볼루션 레이어를 거치면서 외곽 데이터가 유실되는것을 최소화할 수 있는 기능이다.
데이터의 겉에 0의 데이터를 채워넣어 버려지는 데이터를 최소화시킴.
same 함수를 넣으면, 행과 열에 커널사이즈 만큼의 0의 데이터가 추가되므로 레이어를 거쳐도 
행과 열이 수축하지 않는다.
스트라이드는 커널 혹은 맥스풀링의 필터가 움직이는 거리에 대한 파라미터이다.
커널은 기본 1, 맥스풀링은 2가 디폴트이다.


keras32
케라스 데이터셋의 mnist 데이터를 이용한
imshow, cnn, maxpooling 사용, hamsu형 모델 적용 실습


keras33
케라스 데이터셋의 cifar10 데이터를 이용한
imshow, cnn, maxpooling 사용, hamsu형 모델 적용 실습


keras34
케라스 데이터셋의 cifar100 데이터를 이용한
imshow, cnn, maxpooling 사용, hamsu형 모델 적용 실습



keras35
케라스 데이터셋의 fashion mnist 데이터를 이용한
imshow, cnn, maxpooling 사용, hamsu형 모델 적용 실습


keras36 
kaggle house 데이터 모델링

keras37~39
각 데이터를 DNN, CNN, RNN을 이용하여 모델링


keras40
RNN의 한 종류인 LSTM에 관해 배웠다
LSTM은 일반적인 RNN에 비해 4배의 연산량을 가지고 있다.
포겟 게이트, 인풋 게이트, 아웃풋 게이트 총 세개의 게이트와
셀 스테이트라는 한가지 스테이트로
총 네개의 구멍이 있어 연산도 네배로 한다.
성능은 좋지만 연산속도가 느리다는 단점이 있따.


keras41
RNN의 한 종류인 GRU에 관해 배웠다.
GRU는 일반적인 RNN에 비해 3배의 연산량을 가지고 있다.
리셋 게이트, 업데이트 게이트 두개의 게이트와
스테이트 하나로
총 세개의 구멍이 있어 연산을 세배로 한다.
성능이 좋지만 데이터가 많아지면 많아질수록 LSTM이 좀 더 좋다는 연구결과가 있다.
연산을 세배로 하지만 파라미터 계산을 해보면 bias가 하나 더 추가된 것을 알 수 있다(최근 패치)


keras42
return_sequence에 관해 배움
RNN의 특징인 Input3 output2의 특성을,
RNN을 겹쳐 쓰기위해 output을 3으로 맞춰주는 함수이다.
대체적으로 안좋다고는 하지만 딥러닝은 모르는 문제...


keras43
시계열 데이터의 스플릿에 관해 배움.
def 함수 생성, for문을 이용한 데이터 분리에 관해 배웠다.
시계열 데이터는 일반적으로 y값이 지정되지 않아 개발자가 직접 나눠줘야한다.
과정을 편리하게 하기위해 직접 스플릿 함수를 생성했다.
def split_x(dataset, timesteps):
    aaa = []
    for i in range(len(dataset) - timesteps + 1):
        subset = dataset[i : (i + timesteps)]
        aaa.append(subset)
    return np.array(aaa)

for 문의 첫줄은 반복할 횟수에 관한 내용.
그 다음은 반복할 함수에 관한 내용,
return은 최종 출력값에 대한 내용이다.

함수 사용 후 리스트화 된 데이터셋에서 x와 b로 나누어주었다

ex
x = bbb[:, :4]   #콤마 다음은 열에 대한 이야기 이다
x = bbb[:, :-1]  #모든행, 마지막 열 전'까지'
y = bbb[:, -1]   #모든 행, 마지막 열


keras44
rnn을 이용하여 dnn에서 수업했던 데이터셋들을 모델링 해보았다.
일반적으로 성능이 잘 나오지 않았다.
애초에 데이터셋이 시계열 데이터가 아니라 그렇다.

keras45
rnn을 이용하여 cnn에서 수업했던 데이터셋들을 모델링 해보았다.
역시 성능이 좋지 않았다. 시간도 너무 오래걸린다.

keras46
Bidirectional에관해.
RNN을 보조해주는 함수. RNN과 함께 써야한다. (wrapping 한다고 한다)
Bidirectional의 첫번째 파라미터는 rnn같은 함수이고, 두번째 파라미터는 인풋 쉐이프
RNN을 한번 더 순환 시켜주는 함수

keras47
Conv1D에 관해.
Conv1D의 파라미터 계산은 Conv2D와 같다
Conv1D는 LSTM이 값을 다음으로 넘기는 것과 달리 특성을 추출하여 연산한다.
Conv1D는 3차원 입력 3차원 출력.


keras48~49
DNN과 CNN의 데이터를 Conv1D로 모델링한다.