분류문제에서 딥러닝은 layer를 거쳐 선을 수정하지만, 머신러닝은 단층 레이어로 선을 그어 클래스를 분류한다.
때문에 고도화된 문제는 완벽한 분류를 위해 개발자가 파라미터 조정을 해야한다.
or, and, xor 문제 에서
or, and는 하나의 선으로 해결 가능하지만,
xor 문제는 하나의 선으로 해결 불가능.
때문에 단층 퍼셉트론을 사용하던 기존과 달리 다층 퍼셉트론이 등장하기 시작
###############################################################


###############################################################
all_estimators
from sklearn.utils import all_estimators
모든 모델에 대한 평가
allAlgorithms = all_estimators(type_filter='regressor')
allAlgorithms = all_estimators(type_filter='classifier')
max_r2 = 0
max_name = '바보'
for (name, algorithm) in allAlgorithms:
    try: #에러 예외처리
        model = algorithm()
        #3. 훈련
        model.fit(x_train, y_train)
        
        #4. 평가, 예측
        results = model.score(x_test, y_test)
        print(name,'의 정답률 :', results)
        if max_r2 < results :
            max_r2 = results
            max_name = name
        # y_predict = model.predict(x_test)
        # # print(y_test.dtype)  #데이터 타입 확인
        # # print(y_predict.dtype) #데이터 타입 확인    데이터 타입 변경은 astype 사용함.
        # r2 = r2_score(y_test, y_predict)
        # print('r2_score :', r2)
    except:
        #에러가 뜨면 except로 바로 넘어간다. 에러가 안뜨면 정상적으로 for문이 돌아감.
        print(name, '은(는) 에러뜬 놈!!!')
        #에러가 뜨는 모델들은 기본적으로 파라미터 수정이 필요한 모델들이다.
print('===================================')
print('최고모델 :', max_name, max_r2)
print('===================================')
###############################################################




###############################################################
k폴드
from sklearn.model_selection import KFold, cross_val_score
kfold = KFold(n_splits = n_splits, shuffle=True, random_state=123)
for i, datasets in enumerate(datasets):
    x,y = datasets
    print(f"\n데이터셋 {i+1}:")
    scores = cross_val_score(model, x, y, cv=kfold, n_jobs=-1)   # n_jobs= 사용할 코어 갯수
    print('ACC :', scores, '\ncross_val_score 평균 : ', round(np.mean(scores), 4))

kfold = KFold() 디폴트가 5. 데이터를 훈련시키는 위치에 따라서 성능차이 엄청남
기존에는 test 데이터를, train데이터와 다른 데이터로 구분함으로써 평가 용도로 사용하였는데 이를 구분없이 사용하면 이 또한 과적합이라 볼 수 있다.
따라서 테스트 데이터를 분리한 후, cross_val 시켜주는 방법도 있다.

일반적인 kfold의 문제점
문제점 1. train, test/ test로 predict 한 것이므로 과적합만큼 결과의 acc가 안나올 수 있음
문제점 2. stratify kfold 과정중에 y값이 편향될 수 있다.
StratifiedKFold = 비율대로 잘라주는 kfold. 위 문제점을 해결할 수 있다.
분류 문제에서만 사용함.
###############################################################


###############################################################
from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, StratifiedKFold 

GridSearch
파라미터 전체 조사
대부분의 파라미터는 모델과 fit에서 정의함
gridSearch는 모든 파라미터를 돌려본다

gridsearch 포문 구현
for i in gamma:
    for j in C:
        #2. 모델
        model = SVC(gamma=i, C=j)
        
        #3. 컴파일, 훈련
        model.fit(x_train, y_train)
        
        #4. 평가, 예측
        #텐서플로의 evaluate
        #사이킷런의 score
        score = model.score(x_test, y_test)
                
        if max_score < score :
            max_score = score
            best_parameters = {'gamma' : i, 'C' : j} # if문은 if문끼리 묶여있다. 점수가 갱신될때만 best_parameters도 갱신된다.
gridSearch는 crossval 기능도 있다


랜덤서치
from sklearn.model_selection import RandomizedSearchCV
RandomSearch는 fold 하나당 x개씩만 랜덤하게 뽑아서 훈련

할빙그리드서치 
정해진 알고리즘대로 뽑아서 훈련함
from sklearn.model_selection import HalvingGridSearchCV

###############################################################

from sklearn.pipeline import make_pipeline

전처리와 모델을 한번에 사용하는 법
model = make_pipeline(StandardScaler(), SVC())

for i in range(len(data_list)):
    x, y = data_list[i](return_X_y=True)
    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state= 337, stratify=y)
    max_score = 0
    
    for j, value2 in enumerate(scaler_list):
        
        for k, value3 in enumerate(model_list):
            
            model = make_pipeline(value2, value3)
            model.fit(x_train, y_train)
            score = model.score(x_test, y_test)
            y_pred=model.predict(x_test)
            acc=accuracy_score(y_test, y_pred)
            
            if max_score < score:
                max_score = score
                max_s_name = scaler_name[j]
                max_model_name = model_name_list[k]

포문에 넣을수도 있음

PCA사용 예
model = make_pipeline(PCA(n_components=8),StandardScaler(), SVC())

일반 파이프라인
from sklearn.pipeline import Pipeline
model = Pipeline([("std",StandardScaler()), ('svc',SVC())])   
파이프라인은 리스트 형태로 입력해야함
파이프라인은 튜플 형식, 이름 지정까지 함께 입력해야한다.

3중 포문 파이프라인
for i in range(len(data_list)):
    x, y = data_list[i](return_X_y=True)
    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state= 337, stratify=y)
    max_score = 0
    
    for j, value2 in enumerate(scaler_list):
        
        for k, value3 in enumerate(model_list):
            
            model = Pipeline([('scaler',value2), ('model',value3)])
            model.fit(x_train, y_train)
            score = model.score(x_test, y_test)
            y_pred=model.predict(x_test)
            acc=accuracy_score(y_test, y_pred)
            
            if max_score < score:
                max_score = score
                max_s_name = scaler_name[j]
                max_model_name = model_name_list[k]

파이프라인 그리드서치
pipe = Pipeline([("std",StandardScaler()), ('rf',RandomForestClassifier())])
model = GridSearchCV(pipe, parameters,
                     cv = 5,
                     verbose=1,
                     n_jobs=-1
                     )
###############################################################


피쳐 임포턴스
feature_importances_
컬런의 종류에 따라 훈련 결과에 악영향을 끼치는 불필요한 컬런이 있다.
때문에 컬런을 걸러내는 작업을 함.
ex)PCA
for i,v in enumerate(models):
    model = v
    model.fit(x_train, y_train)
    result = model.score(x_test,y_test)
    y_predict = model.predict(x_test)
    acc = accuracy_score(y_test, y_predict)
    print(model_name[i], ':', "acc", acc)
    if i !=3:
        print(model, ':', '컬럼별 중요도',model.feature_importances_)
    else :
        print('XGBClassifier()', model.feature_importances_)
    print('----------------------------------------------')


컬런 중요도에 대한 그래프 그리는법
def plot_feature_importances(model):
    n_features = datasets.data.shape[1]
    plt.barh(np.arange(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), datasets.feature_names)
    plt.xlabel('Feature Importances')
    plt.ylabel('Features')
    plt.ylim(-1, n_features)
    plt.title(model)


컬런 중요도 그림으로 확인/ 컬런 삭제하는 for문
for i in range(4):
    globals()['model'+str(i)] = model_list[i]
    globals()['model'+str(i)].fit(x_train, y_train)
    plt.subplot(2, 2, i+1)
    # print(globals()['model'+str(i)].feature_importance_)
    plot_feature_importances(globals()['model'+str(i)])
    if i == 3:
        plt.title('XGBClassifier()')
# plt.show()
for i, v in enumerate(model_list):
    model = v
    model.fit(x_train,y_train)
    y_pred = model.predict(x_test)
    acc = accuracy_score(y_pred, y_test)
    print(i+1,'.', model_name_list[i])
    print('기존 acc :', acc)
    
    a = model.feature_importances_
    a = a.argmin(axis=0)
    
    x_d = pd.DataFrame(x).drop([a], axis=1)
    
    x_train_d, x_test_d, y_train_d, y_test_d = train_test_split(
        x_d, y, train_size=0.8, shuffle=True, random_state=337)
    
    x_train_d = scaler.fit_transform(x_train_d)
    x_test_d = scaler.transform(x_test_d)
    
    model.fit(x_train_d, y_train_d)
    result = model.score(x_test_d, y_test_d)
    print(f'{a}컬럼삭제 후 acc', result)

#######################################################
상관계수
시본으로 상관계수 히트맵 그리는법

print(df.corr())
import matplotlib.pyplot as plt
import seaborn as sns
# sns.set(font_scale=1.2)
sns.heatmap(data=df.corr(), square=True, annot=True, cbar=True)

plt.show()


상관계수를 확인하고 PCA 적용.
PCA = 주성분분석, 차원축소, 컬런 압축,
일반적으로 x만 적용
x에 사용하여 새로운 컬런을 만들기 때문에 비지도 학습으로 분류
스케일링 개념

PCA 포문
for i in range(9):
    pca = PCA(n_components=10-i)
    x =pca.fit_transform(x)
    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=123,shuffle=True,)
    # 2. 모델구성
    model = RandomForestRegressor(random_state=123)

    # 3. 훈련
    model.fit(x_train, y_train)
    results = model.score(x_test, y_test)
    print('차원', i,'개축소', '결과 :',results)


PCA EVR 확인법. 원본 일치율이라고 보면 된다.
x번의 PCA까지 얼마나 원본과 일치한지 확인하는 방법

pca_EVR = pca.explained_variance_ratio_
# print(pca_EVR)


# print(np.cumsum(pca_EVR))
pca_cumsum = np.cumsum(pca_EVR)
import matplotlib.pyplot as plt
plt.plot(pca_cumsum)
plt.grid()
plt.show()


LDA 선형 판별 분석
LDA는 각 데이터의 클래스별로 매치를 시킨다.
LDA는 지도학습이다.
LDA EVR
for i, v in enumerate(datasets):
    x, y = v(return_X_y=True)
    x_lda = lda.fit_transform(x, y)
    print(dataname[i], ':', x.shape, '->', x_lda.shape)
    lda_EVR=lda.explained_variance_ratio_
    cumsum = np.cumsum(lda_EVR)
    print(cumsum)
    print('========================================')



###################################################
결측치 처리에 관한 내용
임퓨터
from sklearn.experimental import enable_iterative_imputer 
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer #결측치에 대한 책임을 돌린다


결측치 확인법
def outliers(data_out):
    quartile_1, q2, quartile_3 =np.percentile(data_out, [25, 50, 75])
    print("1사분위 : ", quartile_1) # 4
    print("q2 : ", q2) # 7
    print("3사분위 : ", quartile_3) # 10
    iqr = quartile_3 - quartile_1  #6
    print("iqr : ", iqr)
    lower_bound = quartile_1 - (iqr * 1.5) # -5
    upper_bound = quartile_3 + (iqr * 1.5) # 19
    
    return np.where((data_out>upper_bound) | (data_out<lower_bound)) #where = if와 비슷한 개념. 입력한 값을 배출한다

# IQR 의 1.5배수를 하는 이유는 중위 범위의 50% 안팍의 데이터는 정상 값으로 본다는 의미이다.
# 따라서 데이터마다 다른 배수값을 지정할수도 있다.

    
outliers_loc = outliers(aaa)
print("이상치의 위치 : ", outliers_loc)

import matplotlib.pyplot as plt
plt.boxplot(aaa)
plt.show()

EllipticEnvelope로 결측치 확인하는법
outliers = EllipticEnvelope(contamination=.3) #contamination = 이상치 범위 지정 파라미터
outliers.fit(aaa)
results = outliers.predict(aaa)
print(results)


############################################################

지정 파라미터 범위내 최대값 찾는 알고리즘

m61참조.
optimizer = BayesianOptimization(f = lgb_hamsu,
                                 pbounds=bayesian_params, #사용할 파라미터 범위
                                 random_state=337,
                                 )
n_iter = 100
stt = time.time()
optimizer.maximize(init_points=5, n_iter = 100)  #init_points 초기 시작점 갯수
ett = time.time()
print(optimizer.max)
print(n_iter, '번 걸린 시간 :',round(ett-stt,2))


f에는 반드시 모델을 함수화 하여 입력해야함


최소값을 찾는 알고리즘

import hyperopt
from hyperopt import hp, fmin, tpe, Trials, STATUS_OK #fmin은 최소값을 찾는 함수

m62 참조



#############################################################
새로운 스케일러 종류
QuantileTransformer
QuantileTransformer(n_quantiles=10)

각 스케일러 포문으로 사용하고 결정계수 뽑는 포문
for d in data_list:
    max_ACC = -1
    max_scaler = None
    if d == 'd_wine' or d == 'ddiabete':
        x, y = data_list[d]
        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, shuffle = True, random_state = 1234)
    elif d == 'iris' or d == 'cancer' or d == 'wine' or d == 'digits' :
        x, y = data_list[d](return_X_y = True)
        x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, shuffle = True, random_state = 1234)
    
    for s in scaler_list:
        scaler = scaler_list[s]
        x_train = scaler.fit_transform(x_train)
        x_test = scaler.transform(x_test)
        model = RandomForestClassifier()
        model.fit(x_train, y_train)
        y_predict = model.predict(x_test)
        acc = np.round(accuracy_score(y_test, y_predict),2)
        if acc > max_ACC:
            max_ACC = acc
            max_scaler = s
        print(f'데이터 : {d}, 스케일러 : {s}, 정확도 : {acc}')
    print(f'데이터 : {d}, 가장 높은 결정 계수 : {max_ACC}, 가장 높은 결정 계수 스케일러 : {max_scaler}')


############################################################

대형 데이터에 대한 log 변환

describe와 히스토그램(hist)확인 후
log 변환하고, 값을 비교해본다.

log는 0을 계산할 수 없으므로 np.log1p (1더하기)를 사용한다.
분류형은 로그변환 불가능

r2 = r2_score(np.expm1(y_test), np.expm1(model.predict(x_test))) 
#로그변환된 값을 다시 지수변환 하고 비교해야한다

############################################################

다중공선성

# 컬런의 상관관계에 관한 내용
# 상관관계가 너무 높으면 제거하거나 차원 축소하는것이 나을지도 모른다
# 코릴레이션(corr)과 비슷한것
# 다중공선성 확인을 위해서는 스케일링을 먼저 해준다.(컬런이 다르면 값들의 차이가 크기 때문에)

scaler = StandardScaler()
# 다중공선성은 통상 스탠다드 스케일러 사용한다

vif = pd.DataFrame()
vif['variables']=x.columns #컬런 이름 넣기. 3개

        # aaa for i in range(x_scaled.shape[1])  #포문의 내용이 aaa에 들어간다
vif['VIF'] = [variance_inflation_factor(x_scaled, i) for i in range(x_scaled.shape[1])] #vif의 VIF에는 포문의 반환값 세개가 들어가있다.
ㅇ

