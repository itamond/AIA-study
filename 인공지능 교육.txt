아나콘다, visual studio code 설치
폴더설정 -> 보기-> 파일 확장명, 숨긴 항목 체크




pip list -  아나콘다 설치 프로그램 버전확인

pip install tensorflow - 텐서플로 설치 

import 포함시켜라

import tensorflow as tf -> 텐서플로를 포함시키는데 tf라고 부를거다.

print(tf.__version__) 텐서플로 버전 출력

import numpy as np - >numpy  라는 글자 형식을 불러옴

x = np.array([1,2,3])  -> x는 1과 2와 3이다 np 형식으로 불러온.

numpy = 인간이 하는 계산과 매우 비슷. 그래서 많이 쓴다.


# = 주석 , 이 코드는 실행시키지 않음. 설명문


"" 따옴표 -> 따옴표 안의 글씨를 그대로 출력

print("a와 b를 더한 값 : ", c)
'-> a와  b를 더한 값 : c 로' 출력



print(x.shape) = 모양을 확인하는 함수. 모델링 전에 많이 사용

optimizer = 최적화

Sequential 모델 =  순차적 모델.

Dense = 이전층의 노드를 다음 층의 모든 노드에 훈련
evaluate= 평가 함수. fit에서 생성된 w 값에 x와 y데이터를 넣어서 판단을 하는것이다
predict= 예측 함수 위에서 []라는 데이터를 넣었으므로 여기서도 []를 넣는다



인공지능은 y = ax + b 이다.
인공지능에서는 wx + b 라고 한다.
w = weight

인공지능은 이 w값을 찾는 것.

인공지능을 학습 시킬때 생기는 오차 (error, loss, cost, 비용, 오차)

최소의  loss를 만드는 것이 인공지능의 궁극의 목적

"최소의 loss를 구해서"->"최적의 weight를 찾는것"

그래프에 선을 긋는 행위(랜덤하게) -> 예측 모델을 만든다.

히든레이어를 가장 효율적으로 찾는 과정
딥러닝은 발명이 아닌 가장 효율적인 히든 레이어를 '발견'하는 과정


"텐서 1에서는 코스트
텐서 2이상부터는 로스"



뉴런= 노드 node

AI > ML > DL


(눈)   - ( 뇌 : 뉴런-뉴런-뉴런-뉴런) - (입에서 침을 흘리다)  (딥러닝의 예제)
input                                      output
 각각의 뉴런마다 y=wx+b 를 했더니 성능이 더 좋다.



반복된 훈련마다
새로운 훈련을 시도할때 w값이 갱신된다(역전파)


이 순서대로 코딩! (AI개발의 진리)
1.데이터  (데이터를 준비하고 깔끔하게정제하는 과정) **가장 힘든일, 가장 중요한 일**
2.모델구성
3.컴파일, 훈련
4.평가, 예측


batch
1,2,3,4,5 를 훈련할때 5번 각각 훈련 -> 1epoch


batch 1이면 1,2,3,4,5 각각 훈련
batch 2이면 12,34,5 로 훈련
batch 3이면 123,45 로 훈련
batch는 developer 재량
batch는 작게 자르면 좋다
그러나 시간이 오래걸린다
batch_size= 핏(fit)에서 수행



행렬


1, 2, 3 이 있을때 각각의 하나의 숫자 = 스칼라 scalar   0차원이다.

[1, 2, 3] 벡터 1차원이다.

[[1, 2, 3]. [ ]]  벡터가 모이면 행렬이라고 한다 metrix 2차원이다.

대괄호 하나당 차원이 하나씩 늘어난다

3차원부터 Tensor 라고 부른다
4차원= 4차원 tensor




1. [[1, 2], [3, 4]]   2X2  2행 2열
2. [[[1, 2, 3]]]   1X1X3  1행 3열자리가 1개
3. [[[1, 2, 3], [4, 5, 6]]]     1X2X3 2행 3열짜리가 1개
4. [[1], [2], [3]]   3X1   3행 1열
5. [[[1]], [[2]], [[3]]]   3X1X1    1행 1열짜리가 3개
6. [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]  2X2X2   2행 2열짜리가 2개
7. [[[1, 2]], [[3, 4]], [[5, 6]], [[7, 8]]]  4X1X2  1행 2열짜리가 4개




1, 2, 3 각각은 스칼라   0차원
[1, 2, 3]    <-벡터는 (3, ) 라고 표시 (3 콤마)   1차원
[[1,2,3,],[4,5,6]] < 행렬  (2, 3)   2차원
x4 => Tensor(4,2,3)    4 바이 2 바이 3   3차원     -> 4 면 2 행 3 렬
x5 => 4차원 Tensor(5,4,2,3) 5바이 4바이 2바이 3



행은 데이터의 갯수, 열은 데이터의 특성★
!!!행무시, 열우선!!!
모델링을 할때 '열의 갯수'를 판단하게 될 것
데이터 (행)을 추가해도 프로그램은 잘 돌아감.


로스를 계산하는 방법
mae = 절대값
mse = 제곱



분류 (남자or여자 같이 0과 1로 구분할 수 있는)

2진분류, 다중분류

회귀 (수치로 나오는)




warning = 경고는 하지만 작동은 한다.   error = 돌아가지 않음

r2는 보조지표이다. 때문에 loss와 병행해서 판단해야한다.




verbose = 말 수가 많다
fit과 evaluate에서 수행
#verbose 0 ->훈련과정 삭제
#verbose의 디폴트는 1이다. 다 보여준다.
#verbose 2는 진행(프로그레스)바가 없다.
#verbose 나머지는 에포만 나온다





keras.io
https://keras.io/api/models/model_training_apis/
에서 참고하면 좋다.


**캐글->개발자들의 놀이터**
kaggle



dacon.io

*데이콘*



RMSE = root mse


'빅데이터 분석기사'




. = 현 폴더
/ = 아래
[] = 자료형 리스트   

'두 개 이상은 리스트'


데이터를 받았을때 type, shape, describe, info, feature_names 는 항상 확인하자


통상적인 테스트에서는 train 파일만으로 분리해서 훈련한다.





***train_csv데이터에서 x와 y를 분리!!*** 매우 중요
x=train_csv.drop([count], axis=1)        #train_csv에서 count 라는 열(axis=1)을 분리
y=train_csv['count']                        #train_csv에서 count 라는 열을 지정




결측 데이터

중요한 대회에서는 결측 데이터를 고의로 만들고 맞춰라는 문제도 있음.

결측치 처리의 첫번째

0으로 채운다 (부정확하다)

********************결측치 처리하는 방법******************
#통 데이터일 때 결측치 처리를 한다. 분리 후 결측치 처리를 하게되면 데이터가 망가진다.

# 결측치 처리 방법
1. 제거                (결측된 데이터의 행 자체가 사라지기 때문에 아까운 방법)
#print(train_csv.isnull())   # isnull   -> 데이터가 null값인가요? 하고 물어보는 함수
print(train_csv.isnull().sum())  #isnull의 트루값이 몇개인지에 대한 합계(sum) ************자주 사용한다.
train_csv = train_csv.dropna()   #dropna = 결측치 삭제 함수*****
print(train_csv.isnull().sum())
print(train_csv.info())          #데이터 몇개가 남았는지 확인 
print(train_csv.shape)           #(1328, 10)


# loss가 nan으로 뜨는 이유 = 결측치가 너무 많다.


따릉이파일 모델링 과정


#1. 데이터 

-> 경로 지정 불러오기

path = './_data/ddarung/'      .은 현 디렉토리 선택 path라는 변수로 지정하여 추후에 사용하기 용이하게 지정함.

train_csv = pd.read_csv(path+ 'train.csv', index_col=0)     pd.read_csv 함수로 train.csv를 index_col=0으로 지정하여 불러오기

print(train_csv.isnull().sum())    을 통해 isnull의 True 값을 합산한 결과를 본다.

train_csv=train_csv.dropna()    을 통해 train_csv의 결측값을 drop한다.








판다스
pd.read_csv
info()
describe()




윤영선 강사님 email
kingkeras@naver.com


데이터의 분리
train, test, val       -> train에 영향주는 데이터
predict(y가 없는 데이터)-> train에 영향을 주지 않는 데이터

네가지로 분리 가능



model.fit 은 출력하면 정보만 나온다. fit의 출력 데이터는 history에 저장된다.
따라서 프린트를 위해서는 fit을 변수 저장하고 .history를 붙여 history 데이터를 불러온다.




모델에 많은 epochs를 주고 돌리다보면, 일정 부분 이후에서는 그래프가 요동치는 부분이 있다.(overfit,과적합)






val_loss는 모의고사라 생각하면 된다. loss값보다 val_loss가 떨어지면 val_loss가 더 신뢰됨





소문자로 시작되면 함수,
대문자로 시작하면 클래스     둘의 차이점을 보고 느낀점을 메일로 보낸다.
->암묵적인 약속이므로 바뀔수도 있다.






EarlyStopping  = 최소 loss값에서 x번 에포  갱신이 안되면, 그 최소값에서 자른다.
                갱신되면 다시 x번  갱신




*강제성 없는 내용*
EarlyStopping < 대문자로 씌여진 부분(의미가 생기는 부분)
                         낙타와 닮았다 해서 카멜 케이스라고 함

train_test_split < 뱀처럼 생겼다 해서 스네이크 케이스
가능하면 언더바 이용한 스네이크 케이스로 만들자. 가독성이 좋다.




시각화
# plt.rcParams['font.family']='Malgun Gothic'    #폰트변경
# plt.figure(figsize=(9,6))     #시각화 사이즈
# plt.title='쩌는 캐글 바이크'   #시각화 타이틀
# plt.plot(hist.history['loss'], c='red', label='로쓰', marker='.')  #시각화 선긋기
# plt.plot(hist.history['val_loss'], c='blue', label='발_로쓰', marker='.')
# plt.grid()   #격자 넣기
# plt.legend() #데이터 주석
# plt.show()   #show 해라





리스트 = [1,2,3,4,5,1,2,3,4,1,]   #대괄호
['바보','바ㅏ바','안바보'] 등등 쭉 이어져 있는 형태

튜플 = (3123,4123,4321) 튜플은 변경할 수 없다   #소괄호

딕셔너리 = { 앞에 뭔가 있고 : 이후 값이 입력되어 있음         #키 : 벨류 구조    /   중괄호
    {'데이터' : [389217,2134,123,4,123]} 이런식
    진짜 '사전'식, 키와 벨류 한쌍으로 되어있음








이진분류는 무조건 두가지만 기억하자.

**이진분류 데이터가 나오면 , 마지막 레이어에 activation = 'sigmoid' 준다.**
sigmoid = 0과 1사이로 데이터를 한정시키는 활성화 함수

**loss를   loss='binary_crossentropy' 를 준다. **
  mse로 판단하면 실수로 뜨기 때문에 안된다. 
 (예측값과 실제값의 오차인것은 똑같지만 표현, 계산 방식이 다르다.)

분류모델 -> accuracy score


np.round = 반올림 함수




r2는 회귀모델에 씀



데이터의 비율만큼 train_test_split 해주기


softmax = 각 라벨에 대한 값의 합은 항상 1이다
                확률값으로 뽑는 함수이다. 분류하는 라벨에 대한 확률을 부여한다.


원 핫 인코더
값의 value가 아닌 위치로 판단을 하게 한다.
          0        1        2       sum
가위0     1        0        0        1
바위1     0        1        0        1
보   2    0        0        1        1

가위 = [1, 0, 0]
바위 = [0, 1, 0]
보   =  [0, 0, 1]

라벨의 갯수만큼 shape가 늘어난다




원래 y로 predict한 값을 one-hot 인코딩 한 값과 비교

ex) [0.1, 0.7, 0.2]      <->      [0, 1, 0]
이때의 로스는 categorical_crossentropy



배치사이즈가 크다면 메모리에 더 부담이간다.
ex) 1배치에 1mb라면, 10배치에는 10mb를 한번에 던지니까 그런것




## 원하는 버전의 api 설치 pip uninstall scikit-learn==(버전명입력)




#넘파이는 부동소수점 연산(실수)에 좋다.
#정규화 : 0부터 100만까지의 숫자를 0부터 1까지의 숫자로 바꿔줄 수 있다.  (100만으로 나누기)
#우리가 받는 데이터는 항상 x와 y값을 받고 시작한다. (그 값을 토대로 fit을 하기때문)
#정규화 = nomalization (원래 단어 뜻은 다름)
#모든 x데이터를 0에서 1 사이로 만들어버린다 (y는 제외!!)
#과부화걸리지 않게함, 속도를 빠르게함, 성능도 좋아질 '수도' 있다**
#x가 나누어지더라도 가르치는 y는 동일하다. 
#(x-min)/(max-min)



인공지능 모델은 두개밖에 없다.

회귀모델, 분류 모델

y값에 따른 모델 분류
y값이 수치로 나온다 (선형회귀 모델)
y값이 클래스,라벨로 나온다 (분류 모델)





scaler = StandardScaler()
# scaler.fit(x_train)   
# x_train = scaler.transform(x_train)

x_train= scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)            
print(np.min(x_test), np.max(x_test))   









******model.save(경로)*******
세이브 모델은 모델을 저장하는것 뿐만 아니라 가중치도 저장한다.
모델 구성 다음에 저장하면 모델만 저장,
fit 뒤에서 save 하면 가중치도 저장. 때문에 w가 고정된다.

*******model = load_model(경로)*********




색은 숫자가 높을수록 밝은 색. 255






문장 블럭치고 shift+tab = 왼쪽으로 보내기
                    tab = 오른쪽으로 보내기





